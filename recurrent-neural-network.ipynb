{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "## Seoul AI Meetup, August 5\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "### Books\n",
    "* Hands-On Machine Learning with Scikit-Learn and Tensorflow (Chapter 14. Recurrent Neural Networks)\n",
    "    * https://www.safaribooksonline.com\n",
    "    * https://github.com/ageron/handson-ml\n",
    "* Deep Learning Book (Chapter 10: Sequence Modeling: Reccurent and Recursive Nets)\n",
    "    * http://www.deeplearningbook.org/\n",
    "    * https://github.com/HFTrader/DeepLearningBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Videos\n",
    "* [CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM (Andrej Karpathy)](https://www.youtube.com/watch?v=yCC09vCHzF8&t=1s)\n",
    "* [Lecture 8: Recurrent Neural Networks and Language Models (Richard Socher)](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
    "* [Deep Learning Summer School 2016, Recurrent Neural Networks (Yoshua Bengio)](http://videolectures.net/deeplearning2016_bengio_neural_networks/)\n",
    "* [Ch 10: Recurrent/Recursive Nets, DeepLearning Textbook Study Group (Jeremy Howard)](https://www.youtube.com/watch?v=o2QuErsWp6k)\n",
    "* [MIT 6.S094: Recurrent Neural Networks for Steering Through Time (Lex Fridman)](https://www.youtube.com/watch?v=nFTQ7kHQWtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feed Forward Neural Networks\n",
    "\n",
    "Feed Forward Neural Networks has following limitations.\n",
    "\n",
    "* Inputs and outputs have **fixed size**.\n",
    "* Assume **independence** between input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "* RNN operate over sequences of data.\n",
    "  * **Sequences** in the **inputs**.\n",
    "  * **Sequences** in the **outputs**.\n",
    "  * **Sequences** in both **inputs** and **outputs**.\n",
    "* Weights and biases are shared over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1401.png\" style=\"height: 70%; width: 70%\" />\n",
    "</center>\n",
    "\n",
    "*Left*: Recurrent neural network with one neuron in cell.\n",
    "\n",
    "*Right*: **Unfolded (= unrolled)** recurrent neural network with one neuron in cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation of single RNN cell\n",
    "\n",
    "```python\n",
    "# x represents input data             [batch_size, n_input_features]\n",
    "# h represents hidden state           [batch_size, n_neurons]\n",
    "# W_xh weights applied to input data  [n_input_features, n_neurons]\n",
    "# W_hh weights of hidden state        [n_neurons, n_neurons]\n",
    "# W_hy weights for output             [n_neurons, n_outputs] \n",
    "# activation function tanh squashes data in between [-1, 1]\n",
    "\n",
    "h = np.tanh(np.dot(x, W_xh) + np.dot(h, W_hh))\n",
    "\n",
    "# same as expression above\n",
    "#h = np.tanh(np.dot(np.hstack((x, h)), np.vstack((W_xh, W_hh))))\n",
    "\n",
    "# prediction at current time\n",
    "y = np.dot(h, W_hy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layer of Recurrent Neurons \n",
    "\n",
    "Connections between\n",
    "\n",
    "* input and hidden layer,\n",
    "* hidden layer in time $t_{i}$ and hidden layer in time $t_{i+1}$ and\n",
    "* hidden layer and output layer\n",
    "\n",
    "are **fully connected**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1402.png\" style=\"width: 70%; height: 70%\" />\n",
    "</center>\n",
    "\n",
    "*Left*: Recurrent neural network with cell with 5 neurons.\n",
    "\n",
    "*Right*: **Unfolded (= unrolled)** recurrent neural network with 5 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Memory Cell\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/memory_cell.png\" style=\"height: 50%; width: 50%\" />\n",
    "</center>\n",
    "\n",
    "* Simple recurrent neuron or layer of recurrent neurons.\n",
    "* Memory cell preserves state across time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different inputs and output in RNN architectures\n",
    "\n",
    "* Vector to Vector (Feed Forward Neural Network)\n",
    "* Vector to Sequence (e.g. Image Captioning)\n",
    "* Sequence to Vector (e.g. Sentiment Analysis)\n",
    "* Sequence to Sequence (e.g. Machine Translation)\n",
    "* Synced Sequence to Sequence (e.g. Video Captioning)\n",
    "\n",
    "<center>\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building RNN in Tensorflow\n",
    "\n",
    "1. Manully\n",
    "2. `static_rnn()`\n",
    "3. `dynamic_rnn()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building RNN manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "n_neurons  = 5\n",
    "n_steps    = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# batch of size 4 for two time steps\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "\n",
    "# placeholders for input data at two time steps\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "# weight for input data to cell connection\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_features, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# weight for recurrent connection (t-1 => t)\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# bias\n",
    "b  = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "# tf.matmul(X0, Wx) : [None, n_features] * [n_features, n_neurons] = [None, n_neurons]\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "\n",
    "# tf.matmul(Y0, Wy) : [None,   n_neurons] * [n_neurons, n_neurons] = [None, n_neurons]\n",
    "# tf.matmul(X1, Wx) : [None,  n_features] * [n_neurons, n_neurons] = [None, n_neurons]\n",
    "# b : [1, n_neurons]\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def process_batches(X0_batch, X1_batch):    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    \n",
    "    print(\"Y0\\n\",   Y0_val)\n",
    "    print(\"\\nY1\\n\", Y1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y0\n",
      " [[-0.0664006   0.96257669  0.68105787  0.70918542 -0.89821595]\n",
      " [ 0.9977755  -0.71978885 -0.99657625  0.9673925  -0.99989718]\n",
      " [ 0.99999774 -0.99898815 -0.99999893  0.99677622 -0.99999988]\n",
      " [ 1.         -1.         -1.         -0.99818915  0.99950868]]\n",
      "\n",
      "Y1\n",
      " [[ 1.         -1.         -1.          0.40200216 -1.        ]\n",
      " [-0.12210433  0.62805319  0.96718419 -0.99371207 -0.25839335]\n",
      " [ 0.99999827 -0.9999994  -0.9999975  -0.85943311 -0.9999879 ]\n",
      " [ 0.99928284 -0.99999815 -0.99990582  0.98579615 -0.92205751]]\n"
     ]
    }
   ],
   "source": [
    "process_batches(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building RNN using `static_rnn()`\n",
    "\n",
    "* [tf.contrib.rnn.BasicRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell)\n",
    "* [tf.nn.static_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn) creates one cell per time step.\n",
    "* Each input placeholder (`X0`, `X1`) have to be manually defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.nn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)\n",
    "Y0, Y1 = output_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `static_rnn()` output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y0\n",
      " [[ 0.30741334 -0.32884315 -0.65428472 -0.93850589  0.52089024]\n",
      " [ 0.99122757 -0.95425421 -0.75180793 -0.99952078  0.98202348]\n",
      " [ 0.99992681 -0.99783254 -0.82473528 -0.9999963   0.99947774]\n",
      " [ 0.99677098 -0.68750614  0.84199691  0.93039107  0.8120684 ]]\n",
      "\n",
      "Y1\n",
      " [[ 0.99998885 -0.99976051 -0.06679298 -0.99998039  0.99982214]\n",
      " [-0.65249437 -0.51520866 -0.37968954 -0.59225935 -0.08968385]\n",
      " [ 0.99862403 -0.99715197 -0.03308626 -0.99915648  0.99329019]\n",
      " [ 0.99681675 -0.95981938  0.39660636 -0.83076048  0.79671967]]\n"
     ]
    }
   ],
   "source": [
    "process_batches(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `static_rnn()` with single input placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.nn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def process_batches2(X0_batch, X1_batch):\n",
    "    # source https://github.com/ageron/handson-ml \n",
    "    X0_batch_tmp = X0_batch[:, np.newaxis, :]\n",
    "    X1_batch_tmp = X1_batch[:, np.newaxis, :]\n",
    "    X_batch = np.concatenate((X0_batch_tmp, X1_batch_tmp), axis=1)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "\n",
    "    # Y0 output at t = 0\n",
    "    # Y1 output at t = 0\n",
    "    print(\"Y0\\n\",   np.transpose(outputs_val, axes=[1, 0, 2])[0])\n",
    "    print(\"\\nY1\\n\", np.transpose(outputs_val, axes=[1, 0, 2])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y0\n",
      " [[-0.45652324 -0.68064123  0.40938237  0.63104504 -0.45732826]\n",
      " [-0.80015349 -0.99218267  0.78177971  0.9971031  -0.99646091]\n",
      " [-0.93605185 -0.99983788  0.93088669  0.99998152 -0.99998295]\n",
      " [ 0.99273688 -0.99819332 -0.55543643  0.9989031  -0.9953323 ]]\n",
      "\n",
      "Y1\n",
      " [[-0.94288003 -0.99988687  0.94055814  0.99999851 -0.9999997 ]\n",
      " [-0.63711601  0.11300932  0.5798437   0.43105593 -0.63716984]\n",
      " [-0.9165386  -0.99456042  0.89605415  0.99987197 -0.99997509]\n",
      " [-0.02746334 -0.73191994  0.7827872   0.95256817 -0.97817713]]\n"
     ]
    }
   ],
   "source": [
    "process_batches2(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building RNN using `dynamic_rnn()`\n",
    "\n",
    "* [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "* **No need to unstack, stack and transpose!**\n",
    "* Input `[None, n_steps, n_features]`.\n",
    "* Output `[None, n_steps, n_neurons]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y0\n",
      " [[ 0.80872238 -0.52312446 -0.6716494  -0.69762248 -0.54384488]\n",
      " [ 0.99547106 -0.02155113 -0.99482894  0.17964774 -0.83173698]\n",
      " [ 0.99990267  0.49111056 -0.9999314   0.8413834  -0.9444679 ]\n",
      " [-0.80632919  0.93928123 -0.97309881  0.99996096  0.97433066]]\n",
      "\n",
      "Y1\n",
      " [[ 0.9995454   0.99339807 -0.99998379  0.99919224 -0.98379493]\n",
      " [-0.06013332  0.4030143   0.02884481 -0.29437575 -0.85681593]\n",
      " [ 0.99406189  0.95815992 -0.99768937  0.98646194 -0.91752487]\n",
      " [ 0.95047355 -0.51205158 -0.27763969  0.83108062  0.81631833]]\n"
     ]
    }
   ],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "process_batches2(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variable-Length Input Sequences in Tensorflow\n",
    "\n",
    "* Sentences, video, audio, ...\n",
    "* Parameter `sequence_length` in `dynamic_rnn()` represents the lenghts of input vector.\n",
    "* Outputs of RNN are **zero vectors** for every time step past the input sequence length.\n",
    "\n",
    "```python\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "...\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Initialization of `sequence_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 1 (padded with a zero vector)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 3\n",
    "    ])\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variable-Length Output Sequences in Tensorflow\n",
    "\n",
    "* Output length is **known**.\n",
    "    * Solve similarly as with `output_sequences`.\n",
    "    * Ignore every output past the length of output sequence.\n",
    "    \n",
    "    \n",
    "* Output length is **unknown**.\n",
    "    * Generate EOS (end-of-sequence) token.\n",
    "    * Ignore every output past the EOS token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training RNN in Tensorflow\n",
    "\n",
    "* Backpropagation Through Time (BPTT)\n",
    "    * Forward pass\n",
    "    * Compute cost function $C(Y_0, Y_1, ..., Y_{n-1}, Y_n)$.\n",
    "    * Propagate gradient of cost function through unrolled network.\n",
    "    * Update model parameters using the gradients computed during BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST\n",
    "\n",
    "* Dataset of handwritten digits [0-9]\n",
    "* 28x28 px\n",
    "* Grayscale\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/mnist_example.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "n_steps   = 28\n",
    "n_inputs  = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32,   [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "# states = final outputs, after n_steps = 28\n",
    "# outputs = outputs at every time step => 28 outputs\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "# states variable contains state of RNN cell after n_steps = 28\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1) # only one correct output\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source https://github.com/ageron/handson-ml\n",
    "n_epochs   = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs)) # 150, 28, 28\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        #print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```\n",
    "...\n",
    "97 Train accuracy: 1.0 Test accuracy: 0.9809\n",
    "98 Train accuracy: 0.986667 Test accuracy: 0.9761\n",
    "99 Train accuracy: 0.986667 Test accuracy: 0.9769\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep RNN\n",
    "\n",
    "* Stack  of multiple layers of cells.\n",
    "* [tf.contrib.rnn.MultiRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/deep_rnn.png\" style=\"height: 20%; width: 20%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation of deep RNN in Tensorflow\n",
    "\n",
    "```python\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * n_layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bidirectional Recurrent Neural Networks\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/bidirectional-rnn.png\" style=\"width: 30%; height: 30%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "[tf.contrib.rnn.DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper) applies dropout during both training and testing phase!\n",
    "\n",
    "**Solution**\n",
    "\n",
    "* Create own wrapper.\n",
    "* Create two graphs; one for training, one for testing.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/dropout.jpeg\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout with two graphs\n",
    "\n",
    "```python\n",
    "keep_prob = 0.5\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "if is_training:\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "\n",
    "...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if is_training:\n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            # train the model\n",
    "        save_path = saver.save(sess, \"model.ckpt\")\n",
    "    else:\n",
    "        saver.restore(sess, \"model.ckpt\")\n",
    "        # use the model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN problems\n",
    "\n",
    "With long input sequences RNN suffers from several problems.\n",
    "\n",
    "* Vanishing/Exploding gradients\n",
    "* Non-convergance\n",
    "* Memory of the first inputs fade away\n",
    "* Training of long sequences is slow\n",
    "\n",
    "**Partial solutions**\n",
    "\n",
    "* Good parameter initialization (weights initialized as identity matrix)\n",
    "* Nonsaturating activation functions (e.g., ReLU)\n",
    "* Batch Normalization\n",
    "* Gradient Clipping\n",
    "* Faster optimizers\n",
    "* **Truncated** Backpropagation Through Time => model cannot learn long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM  Cell\n",
    "\n",
    "* [Long Short-Term Memory, S. Hochreiter and J. Schmidhuber (1997)](http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw)\n",
    "* Same inputs and outputs as basic RNN cell, but state is split.\n",
    "* Faster convergence.\n",
    "* Detect long-term dependencies in data.\n",
    "* 4 different fully connected layers\n",
    "* 3 gates (learn what to store in the long-term state, what to throw away, and what to read from it)\n",
    "    * Input\n",
    "    * Forget\n",
    "    * Output\n",
    "* 2 states\n",
    "    * short-term\n",
    "    * long-term\n",
    "\n",
    "\n",
    "* Tensorflow [tf.contrib.rnn.BasicLSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "* Keras [keras.layers.recurrent.LSTM](https://keras.io/layers/recurrent/#lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization of LSTM cell\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_1413.png\" style=\"height: 80%; width: 80%;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Peephole Connections\n",
    "\n",
    "* [Recurrent Nets that Time and Count, F. Gers and J. Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)\n",
    "* In LSTM gate controllers utilize only previous state and current input.\n",
    "* Peephole connections allow them to use (\"peep\") long-term state as well.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/peephole_connections.png\" style=\"height: 50%; width: 50%;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Unit Cell (GRU)\n",
    "\n",
    "* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, K. Cho et al. (2014)](https://arxiv.org/abs/1406.1078)\n",
    "* Simplified version of LSTM cell\n",
    "* Single state vector\n",
    "* Gates (reset and update gate)\n",
    "* Single gate controller (instead of input and forget gate)\n",
    "    *  1 => the **input gate** is **open**, the **forget gate** is **closed**\n",
    "    *  0 => the **input gate** is **closed**, the **forget gate** is **open**\n",
    "\n",
    "\n",
    "* Tensorflow [tf.contrib.rnn.GRUCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GRUCell)\n",
    "* Keras [keras.layers.recurrent.GRU](https://keras.io/layers/recurrent/#gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization of GRU Cell\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/gru.png\" style=\"height: 70%; width: 70%;\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN usage and Examples \n",
    "\n",
    "* Machine Translation\n",
    "* Automatic Summarization\n",
    "* Image/Video Captioning\n",
    "* Sentiment Analysis\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum Binary Numbers\n",
    "\n",
    "* Inspired by [Neural Networks for Machine Learning lecture, (Geoffrey Hinton)](https://www.youtube.com/watch?v=bVGdxHgxG34&t=1s)\n",
    "* [Jupyter notebook with Tensorflow (martinkersner)](https://github.com/martinkersner/rnn-meetup/blob/master/sum-binary-numbers.ipynb)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/binary_addition.png\" />\n",
    "</center>\n",
    "\n",
    "### Tips\n",
    "* Make sure you start to feed from the least significant bit :)\n",
    "* Don't randomly generate training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Character-Level Text Generation\n",
    "\n",
    "* [Blog post (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Source code (Justin Johnson)](https://github.com/jcjohnson/torch-rnn)\n",
    "\n",
    "**Multilayer recurrent neural network** language model with **dropout** regularization. Softmax on the top.\n",
    "\n",
    "Arguments of [LanguageModel](https://github.com/jcjohnson/torch-rnn/blob/master/doc/modules.md#languagemodel):\n",
    "* `idx_to_token`: A table giving the vocabulary for the language model, mapping integer ids to string tokens.\n",
    "* `model_type`: \"lstm\" or \"rnn\"\n",
    "* `wordvec_size`: Dimension for word vector embeddings\n",
    "* `rnn_size`: Hidden state size for RNNs\n",
    "* `num_layers`: Number of RNN layers to use\n",
    "* `dropout`: Number between 0 and 1 giving dropout strength after each RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Latex generation\n",
    "\n",
    "<center>\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/latex4.jpeg\" style=\"height: 80%; width: 80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C code generation\n",
    "\n",
    "```C\n",
    "/*\n",
    " * Increment the size file of the new incorrect UI_FILTER group information\n",
    " * of the size generatively.\n",
    " */\n",
    "static int indicate_policy(void)\n",
    "{\n",
    "  int error;\n",
    "  if (fd == MARN_EPT) {\n",
    "    /*\n",
    "     * The kernel blank will coeld it to userspace.\n",
    "     */\n",
    "    if (ss->segment < mem_total)\n",
    "      unblock_graph_and_set_blocked();\n",
    "    else\n",
    "      ret = 1;\n",
    "    goto bail;\n",
    "  }\n",
    "  segaddr = in_SB(in.addr);\n",
    "  selector = seg / 16;\n",
    "  setup_works = true;\n",
    "  for (i = 0; i < blocks; i++) {\n",
    "    seq = buf[i++];\n",
    "    bpf = bd->bd.next + i * search;\n",
    "    if (fd) {\n",
    "      current = blocked;\n",
    "    }\n",
    "  }\n",
    "  rw->name = \"Getjbbregs\";\n",
    "  bprm_self_clearl(&iv->version);\n",
    "  regs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12;\n",
    "  return segtable;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization of predictions\n",
    "\n",
    "<center>\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/under1.jpeg\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bible generation\n",
    "\n",
    "* [Bible source](http://www.truth.info/download/bible.htm)\n",
    "* 858,195 words\n",
    "* [torch-rnn](https://github.com/jcjohnson/torch-rnn)\n",
    "* 50 epochs\n",
    "\n",
    "**Examples**\n",
    "\n",
    "* *Genesis 39:2*    And the LORD was with Joseph, and he was a prosperous man; and he was in the house of his master the Egyptian.\n",
    "* *Numbers 15:41*   I am the LORD your God, which brought you out of the land of Egypt, to be your God: I am the LORD your God.\n",
    "* *Revelation 22:13*        I am Alpha and Omega, the beginning and the end, the first and the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Generated text**\n",
    "\n",
    "* *Daniel 7:3*      Hear now thine hand shall make him all that I will live from an atonement of his three forth.\n",
    "* *Chronicles 10:22*      Then he searched in the land of Abram the Jedaliah, and the Egyptian, and foods of Jerusalem doth oil.\n",
    "* *Kings 14:17*   And their herds of the holy bones, which I shall deliver juspiah of God upon the LORD, that he had sold the destroying of Jerusalem.\n",
    "\n",
    "> thine = yours\n",
    "\n",
    "> atonement = reconciliation\n",
    "\n",
    "> doth = archaic third person singular present of do\n",
    "\n",
    "> **juspiah** does not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QA bAbI tasks\n",
    "\n",
    "* https://research.fb.com/downloads/babi/\n",
    "* Synthetic dataset of 20 different tasks for testing text understanding and reasoning.\n",
    "\n",
    "Example of task with two supporting facts (QA2):\n",
    "\n",
    "```\n",
    "1 Mary got the milk there.                                                      \n",
    "2 John moved to the bedroom.                                                    \n",
    "3 Sandra went back to the kitchen.                                              \n",
    "4 Mary travelled to the hallway.                                                \n",
    "5 Where is the milk?  hallway 1 4 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question Answering Solution Using Keras\n",
    "\n",
    "http://smerity.com/articles/2015/keras_qa.html\n",
    "\n",
    "Following information are always related to **Two Supporting Facts (QA2)** which can be found in *tasks_1-20_v1-2/en/qa2_two-supporting-facts_[train|test].txt*.\n",
    "\n",
    "* QA2 subdataset contains 1,000 traing and 1,000 testing samples.\n",
    "* The length of stories and questions **differ**.\n",
    "* Test accuracy **31 %**, knowing possible answers (6) accuracy of **random** prediction is **16 %**\n",
    "* [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks, J. Weston et al. (2015)](https://arxiv.org/abs/1502.05698)\n",
    "    * Weakly supervised LSTM, 20 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data Preprocessing\n",
    "```python\n",
    "(# story\n",
    " ['Mary', 'got', 'the', 'milk', 'there', '.',\n",
    "  'John', 'moved', 'to', 'the', 'bedroom', '.',\n",
    "  'Sandra', 'went', 'back', 'to', 'the', 'kitchen', '.',\n",
    "  'Mary', 'travelled', 'to', 'the', 'hallway', '.'],\n",
    " # question\n",
    " ['Where', 'is', 'the', 'milk', '?'],\n",
    " # answer\n",
    "  'hallway')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word vocabulary\n",
    "\n",
    "Only 35 (36) words!\n",
    "\n",
    "```python\n",
    "['.', '?', 'Daniel', 'John', 'Mary', 'Sandra', 'Where', 'apple', 'back', 'bathroom', 'bedroom', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'journeyed', 'kitchen', 'left', 'milk', 'moved', 'office', 'picked', 'put', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Conversion stories to vectors\n",
    "\n",
    "```python\n",
    "# pre-padded with zeros\n",
    "[0 ... 5 17 29 24 30  1  4 25 31 29 11  1  6 35  9 31 29 22  1  5 33 31 29 19  1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applied RNN models\n",
    "\n",
    "Following models can be applied to all bAbI tasks, but have to be trained separately for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model #1 (August 5, 2015)\n",
    "\n",
    "```python\n",
    "sentrnn = Sequential()\n",
    "sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True))\n",
    "sentrnn.add(RNN(EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, return_sequences=False))\n",
    "\n",
    "qrnn = Sequential()\n",
    "qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE))\n",
    "qrnn.add(RNN(EMBED_HIDDEN_SIZE, QUERY_HIDDEN_SIZE, return_sequences=False))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([sentrnn, qrnn], mode='concat'))\n",
    "model.add(Dense(SENT_HIDDEN_SIZE + QUERY_HIDDEN_SIZE, vocab_size, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Architecture\n",
    "\n",
    "<center>\n",
    "<img src=\"http://smerity.com/media/images/articles/2015/keras_qa_model.svg\" style=\"height: 60%; width: 60%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model #2\n",
    "\n",
    "* [keras.layers.add](https://keras.io/layers/merge/#add) sum tensors with same dimensions.\n",
    "* [keras.layers.core.Dropout](https://keras.io/layers/core/#dropout) rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "\n",
    "```python\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')                   \n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)       \n",
    "encoded_sentence = layers.Dropout(0.3)(encoded_sentence)                        \n",
    "                                                                                \n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')                   \n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)       \n",
    "encoded_question = layers.Dropout(0.3)(encoded_question)                        \n",
    "encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n",
    "encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)          \n",
    "                                                                                \n",
    "merged = layers.add([encoded_sentence, encoded_question])                       \n",
    "merged = RNN(EMBED_HIDDEN_SIZE)(merged)                                         \n",
    "merged = layers.Dropout(0.3)(merged)                                            \n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handwriting Generation\n",
    "\n",
    "* [Generating Sequences With Recurrent Neural Networks, A. Graves, 2015](https://arxiv.org/abs/1308.0850)\n",
    "* [Source code](https://github.com/szcom/rnnlib)\n",
    "* [Online demo](https://www.cs.toronto.edu/~graves/handwriting.cgi)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/saim1.jpg\" />\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/saim2.jpg\" />\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/saim3.jpg\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Network Visualization\n",
    "\n",
    "* Window layer as discrete convolution with a mixture of K Gaussian functions.\n",
    "* $Pr(x_{t} | y_{t-1})$ is a multinomial distribution.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/synthesis_network.png\" style=\"height: 40%; width: 40%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Udacity challenge: Prediction of steering angles\n",
    "\n",
    "* Causal predictions = Only past frames are used to predict the future steering decisions.\n",
    "* [Blog post about winning solutions](https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c)\n",
    "* [Source code for all winning solutions](https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models)\n",
    "\n",
    "\n",
    "1. The first place, [Team Komanda solution](https://github.com/udacity/self-driving-car/blob/master/steering-models/community-models/komanda/solution-komanda.ipynb)\n",
    "    * Mapping from sequences of images to sequences of steering angle measurements.\n",
    "    * Applied 3D convolution on input image sequences.\n",
    "    * Then two other layers, **LSTM** and a simple **RNN**, respectively.\n",
    "    * The predicted angle, torque and speed serve as the input to the next timestep.\n",
    "2. The third place, [Team Chauffeur solution](https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/chauffeur)\n",
    "    * Utilized CNN for feature extraction.\n",
    "    * Cropped the top of network in order to get 3,000 features.\n",
    "    * Those features used as input to **LSTM**.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*bYS0BOd1wSS8Hqgdw0Ijrg.gif\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pixel RNN\n",
    "\n",
    "* [Pixel Recurrent Neural Networks, A. Oord et al. (2016)](https://arxiv.org/abs/1601.06759)\n",
    "* Image inpainting, deblurring, generation\n",
    "* The network scans the image one row at a time and one pixel at a time within each row. For each pixel it predicts the conditional distribution over the possible pixel values given the scanned context.\n",
    "* Pixels represented as discrete values using a multinomial distribution implemented with a simple softmax layer.\n",
    "* 12 **LSTM** layers with residual connections.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/rnn-meetup/master/images/pixel_rnn.png\" style=\"height: 50%; width: 50 %;\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
